{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "读取数据第一步，在数据处理文件中，已经将所有信号都补全到150位，长度不足处用-1进行了填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 加载数据\n",
    "def load_data(folder, label):\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            df = pd.read_csv(filepath, usecols=[\"(V)\", \"state\"])  # 仅读取 V 和 state 列\n",
    "            all_data.append(df.values)  # 直接读取为numpy数组\n",
    "            all_labels.append(np.full(len(df), label))  # 为每个样本生成标签\n",
    "    data = np.stack(all_data, axis=0)  # 形状：(样本数, 150, 2)\n",
    "    labels = np.concatenate(all_labels, axis=0)  # 形状：(样本数 * 150,)\n",
    "    return data, labels\n",
    "\n",
    "# 加载正常数据（训练数据）\n",
    "normal_data, normal_labels = load_data(\"padsignal\", label=0)  # 正常数据标签为0\n",
    "\n",
    "# 加载异常数据（验证数据）\n",
    "anomaly_data, anomaly_labels = load_data(\"abpadsignal\", label=1)  # 异常数据标签为1\n",
    "# 假设所有CSV文件已保存在 processed_crcslice 文件夹中\n",
    "# input_folder = \"padsignal\"\n",
    "\n",
    "# # 读取所有CSV文件并合并为数据集\n",
    "# all_data = []\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         filepath = os.path.join(input_folder, filename)\n",
    "#         df = pd.read_csv(filepath, usecols=[\"(V)\", \"state\"])\n",
    "#         all_data.append(df.values)  # 直接读取为numpy数组\n",
    "\n",
    "# data = np.stack(all_data, axis=0)  # 形状：(样本数, 150, 2)\n",
    "\n",
    "# 编码 state 列\n",
    "label_encoder = LabelEncoder()\n",
    "state_column = normal_data[:, :, 1].flatten()  # 提取所有state值\n",
    "label_encoder.fit(state_column)\n",
    "normal_data[:, :, 1] = label_encoder.transform(state_column).reshape(normal_data.shape[0], -1)\n",
    "\n",
    "# # 标准化时间（s）和 V 列（排除填充值-1）\n",
    "# mask = (data[:, :, 0] != -1) & (data[:, :, 1] != -1)  # 掩码非填充区域\n",
    "# scaler = StandardScaler()\n",
    "# data[:, :, 0] = scaler.fit_transform(data[:, :, 0][mask].reshape(-1, 1)).reshape(data[:, :, 0].shape)\n",
    "# data[:, :, 1] = scaler.fit_transform(data[:, :, 1][mask].reshape(-1, 1)).reshape(data[:, :, 1].shape)\n",
    "\n",
    "# 将 data 转换为 float32 类型（确保所有值为数值类型）\n",
    "normal_data = normal_data.astype(np.float32)  # 形状：(样本数, 150, 2)\n",
    "\n",
    "# 转换为PyTorch张量并生成掩码\n",
    "data_tensor = torch.FloatTensor(normal_data)  # 形状：(样本数, 150, 2)\n",
    "mask_tensor = torch.BoolTensor((normal_data[:, :, 0] != -1) & (normal_data[:, :, 1] != label_encoder.transform([\"padding\"])[0]))  # 形状：(样本数, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据集和数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, mask):\n",
    "        self.data = data  # 形状：(样本数, 150, 2)\n",
    "        self.mask = mask  # 形状：(样本数, 150)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.mask[idx]\n",
    "\n",
    "# 划分训练集和验证集（80-20比例）\n",
    "train_size = int(0.7 * len(data_tensor))\n",
    "val_size = int(0.15 * len(data_tensor))\n",
    "test_size = len(data_tensor) - train_size - val_size\n",
    "train_data, val_data, test_data = data_tensor[:train_size], data_tensor[:train_size+val_size], data_tensor[train_size+val_size:]\n",
    "train_mask, val_mask, test_mask = mask_tensor[:train_size], mask_tensor[:train_size+val_size], mask_tensor[train_size+val_size:]\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TimeSeriesDataset(train_data, train_mask)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TimeSeriesDataset(val_data, val_mask)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义LSTM编码器-解码器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncDecAD(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 编码器：输入正向序列，输出最后隐藏状态\n",
    "        _, (h_n, _) = self.encoder(x)\n",
    "        # 解码器：初始状态为编码器最终状态，输入反向序列\n",
    "        reversed_x = torch.flip(x, dims=[1])\n",
    "        output, _ = self.decoder(reversed_x, (h_n, torch.zeros_like(h_n)))\n",
    "        recon = self.linear(output)\n",
    "        return torch.flip(recon, dims=[1])  # 输出与输入同顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义掩码损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(pred, target, mask):\n",
    "    # pred: (batch_size, seq_len, input_dim)\n",
    "    # target: (batch_size, seq_len, input_dim)\n",
    "    # mask: (batch_size, seq_len)\n",
    "    loss = (pred - target) ** 2\n",
    "    loss = loss.mean(dim=2)  # 对特征维度取平均\n",
    "    loss = loss * mask  # 应用掩码\n",
    "    return loss.sum() / mask.sum()  # 仅计算有效区域的损失\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = EncDecAD(input_dim=2, hidden_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | Train Loss: 0.0929 | Val Loss: 0.0010\n",
      "Saved best model.\n",
      "Epoch 2/2 | Train Loss: 0.0002 | Val Loss: 0.0000\n",
      "Saved best model.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_mask in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch_x)\n",
    "        loss = masked_mse_loss(recon, batch_x, batch_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_mask in val_loader:\n",
    "            recon = model(batch_x)\n",
    "            loss = masked_mse_loss(recon, batch_x, batch_mask)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # 打印损失\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # 早停和保存最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理异常数据\n",
    "anomaly_data[:, :, 1] = label_encoder.transform(anomaly_data[:, :, 1].flatten()).reshape(anomaly_data.shape[0], -1)\n",
    "anomaly_data = anomaly_data.astype(np.float32)  # 形状：(样本数, 150, 2)\n",
    "\n",
    "# 转换为PyTorch张量并生成掩码\n",
    "anomaly_data_tensor = torch.FloatTensor(anomaly_data)  # 形状：(样本数, 150, 2)\n",
    "anomaly_mask_tensor = torch.BoolTensor((anomaly_data[:, :, 0] != -1) & (anomaly_data[:, :, 1] != label_encoder.transform([\"padding\"])[0]))  # 形状：(样本数, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建包含异常数据和正常数据的混合数据集\n",
    "mixed_data_tensor = torch.cat((anomaly_data_tensor, val_data), dim=0)\n",
    "mixed_mask_tensor = torch.cat((anomaly_mask_tensor, val_mask), dim=0)\n",
    "mixed_labels = [1] * len(anomaly_data_tensor) + [0] * len(val_data)  # 异常数据标签为1，正常数据标签为0\n",
    "\n",
    "mixed_data_tensor = torch.cat((test_data, anomaly_data_tensor), dim=0)\n",
    "mixed_mask_tensor = torch.cat((test_mask, anomaly_mask_tensor), dim=0)\n",
    "mixed_labels = [0] * len(test_data) + [1] * len(anomaly_data_tensor)  # 正常数据标签为0，异常数据标签为1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.000001 | Precision: 0.0857\n",
      "Threshold: 0.000002 | Precision: 0.0857\n",
      "Threshold: 0.000003 | Precision: 0.0857\n",
      "Threshold: 0.000004 | Precision: 0.0974\n",
      "Threshold: 0.000005 | Precision: 0.2627\n",
      "Threshold: 0.000006 | Precision: 0.2932\n",
      "Threshold: 0.000007 | Precision: 0.3078\n",
      "Threshold: 0.000008 | Precision: 0.3171\n",
      "Threshold: 0.000009 | Precision: 0.3221\n",
      "Threshold: 0.000010 | Precision: 0.3167\n",
      "Threshold: 0.000011 | Precision: 0.3180\n",
      "Threshold: 0.000012 | Precision: 0.3243\n",
      "Threshold: 0.000013 | Precision: 0.3254\n",
      "Threshold: 0.000014 | Precision: 0.3330\n",
      "Threshold: 0.000015 | Precision: 0.3375\n",
      "Threshold: 0.000016 | Precision: 0.3396\n",
      "Threshold: 0.000017 | Precision: 0.3447\n",
      "Threshold: 0.000018 | Precision: 0.3511\n",
      "Threshold: 0.000019 | Precision: 0.3577\n",
      "Threshold: 0.000020 | Precision: 0.3666\n",
      "Threshold: 0.000021 | Precision: 0.3705\n",
      "Threshold: 0.000022 | Precision: 0.3783\n",
      "Threshold: 0.000023 | Precision: 0.3807\n",
      "Threshold: 0.000024 | Precision: 0.3495\n",
      "Threshold: 0.000025 | Precision: 0.3140\n",
      "Threshold: 0.000026 | Precision: 0.3000\n",
      "Threshold: 0.000027 | Precision: 0.2828\n",
      "Threshold: 0.000028 | Precision: 0.2842\n",
      "Threshold: 0.000029 | Precision: 0.2860\n",
      "Threshold: 0.000030 | Precision: 0.2896\n",
      "Threshold: 0.000031 | Precision: 0.2933\n",
      "Threshold: 0.000032 | Precision: 0.3043\n",
      "Threshold: 0.000033 | Precision: 0.3131\n",
      "Threshold: 0.000034 | Precision: 0.3199\n",
      "Threshold: 0.000035 | Precision: 0.3325\n",
      "Threshold: 0.000036 | Precision: 0.3403\n",
      "Threshold: 0.000037 | Precision: 0.3352\n",
      "Threshold: 0.000038 | Precision: 0.3439\n",
      "Threshold: 0.000039 | Precision: 0.3446\n",
      "Threshold: 0.000040 | Precision: 0.3586\n",
      "Threshold: 0.000041 | Precision: 0.3733\n",
      "Threshold: 0.000042 | Precision: 0.3789\n",
      "Threshold: 0.000043 | Precision: 0.3801\n",
      "Threshold: 0.000044 | Precision: 0.3840\n",
      "Threshold: 0.000045 | Precision: 0.3920\n",
      "Threshold: 0.000046 | Precision: 0.3983\n",
      "Threshold: 0.000047 | Precision: 0.4000\n",
      "Threshold: 0.000048 | Precision: 0.4027\n",
      "Threshold: 0.000049 | Precision: 0.4104\n",
      "Threshold: 0.000050 | Precision: 0.4118\n",
      "Threshold: 0.000051 | Precision: 0.4221\n",
      "Threshold: 0.000052 | Precision: 0.4233\n",
      "Threshold: 0.000053 | Precision: 0.4239\n",
      "Threshold: 0.000054 | Precision: 0.4190\n",
      "Threshold: 0.000055 | Precision: 0.4162\n",
      "Threshold: 0.000056 | Precision: 0.4277\n",
      "Threshold: 0.000057 | Precision: 0.4313\n",
      "Threshold: 0.000058 | Precision: 0.4340\n",
      "Threshold: 0.000059 | Precision: 0.4183\n",
      "Threshold: 0.000060 | Precision: 0.4133\n",
      "Threshold: 0.000061 | Precision: 0.4138\n",
      "Threshold: 0.000062 | Precision: 0.4155\n",
      "Threshold: 0.000063 | Precision: 0.4173\n",
      "Threshold: 0.000064 | Precision: 0.4173\n",
      "Threshold: 0.000065 | Precision: 0.4328\n",
      "Threshold: 0.000066 | Precision: 0.4351\n",
      "Threshold: 0.000067 | Precision: 0.4331\n",
      "Threshold: 0.000068 | Precision: 0.4365\n",
      "Threshold: 0.000069 | Precision: 0.4355\n",
      "Threshold: 0.000070 | Precision: 0.4426\n",
      "Threshold: 0.000071 | Precision: 0.4500\n",
      "Threshold: 0.000072 | Precision: 0.4435\n",
      "Threshold: 0.000073 | Precision: 0.4425\n",
      "Threshold: 0.000074 | Precision: 0.4375\n",
      "Threshold: 0.000075 | Precision: 0.4414\n",
      "Threshold: 0.000076 | Precision: 0.4455\n",
      "Threshold: 0.000077 | Precision: 0.4537\n",
      "Threshold: 0.000078 | Precision: 0.4537\n",
      "Threshold: 0.000079 | Precision: 0.4423\n",
      "Threshold: 0.000080 | Precision: 0.4356\n",
      "Threshold: 0.000081 | Precision: 0.4433\n",
      "Threshold: 0.000082 | Precision: 0.4526\n",
      "Threshold: 0.000083 | Precision: 0.4396\n",
      "Threshold: 0.000084 | Precision: 0.4444\n",
      "Threshold: 0.000085 | Precision: 0.4494\n",
      "Threshold: 0.000086 | Precision: 0.4545\n",
      "Threshold: 0.000087 | Precision: 0.4651\n",
      "Threshold: 0.000088 | Precision: 0.4588\n",
      "Threshold: 0.000089 | Precision: 0.4588\n",
      "Threshold: 0.000090 | Precision: 0.4578\n",
      "Threshold: 0.000091 | Precision: 0.4578\n",
      "Threshold: 0.000092 | Precision: 0.4430\n",
      "Threshold: 0.000093 | Precision: 0.4359\n",
      "Threshold: 0.000094 | Precision: 0.4474\n",
      "Threshold: 0.000095 | Precision: 0.4384\n",
      "Threshold: 0.000096 | Precision: 0.4571\n",
      "Threshold: 0.000097 | Precision: 0.4412\n",
      "Threshold: 0.000098 | Precision: 0.4462\n",
      "Threshold: 0.000099 | Precision: 0.4286\n",
      "Threshold: 0.000100 | Precision: 0.4194\n",
      "Best Threshold: 0.0001, Best Precision: 0.4651\n",
      "Final Evaluation - Accuracy: 0.9139, Precision: 0.4651, Recall: 0.0344, F1: 0.0641\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# 创建验证集 DataLoader\n",
    "test1_dataset = TimeSeriesDataset(mixed_data_tensor, mixed_mask_tensor)\n",
    "test_loader = DataLoader(test1_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 验证模型性能\n",
    "def evaluate_model(model, test_loader, threshold, true_labels):\n",
    "    model.eval()\n",
    "    y_true = true_labels\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_mask in test_loader:\n",
    "            recon = model(batch_x)\n",
    "            error = (recon - batch_x) ** 2\n",
    "            error = error.mean(dim=2)  # 对特征维度取平均\n",
    "            error = error * batch_mask  # 应用掩码\n",
    "            anomaly_score = error.sum(dim=1) / batch_mask.sum(dim=1)  # 每个样本的平均重建误差\n",
    "            # print(anomaly_score)\n",
    "            y_pred.extend((anomaly_score > threshold).cpu().numpy().tolist())\n",
    "            # y_true.extend([1] * len(batch_x))  # 异常数据的标签为1\n",
    "    \n",
    "    # 计算评估指标\n",
    "    # print(y_true)\n",
    "    # print(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load(\"best_model1.pth\"))\n",
    "\n",
    "# # 设定阈值（可根据验证集调整）\n",
    "# threshold = 0.0001  # 示例值，需实际调优\n",
    "# evaluate_model(model, test_loader, threshold)\n",
    "# # 加载最佳模型\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "# 寻找最优阈值\n",
    "best_threshold = 0\n",
    "best_precision = 0\n",
    "thresholds = np.linspace(0.000001, 0.0001, 100)  # 示例阈值范围，需实际调优\n",
    "for threshold in thresholds:\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, test_loader, threshold, mixed_labels)\n",
    "    if precision > best_precision:\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "    print(\"Threshold: {:.6f} | Precision: {:.4f}\".format(threshold, precision))\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold:.4f}, Best Precision: {best_precision:.4f}\")\n",
    "\n",
    "# # 设定阈值（可根据验证集调整）\n",
    "# threshold = 0.1  # 示例值，需实际调优\n",
    "# anomalies = detect_anomaly(model, val_data, val_mask, threshold)\n",
    "\n",
    "# print(anomalies)\n",
    "\n",
    "# 使用最佳阈值进行最终评估\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, test_loader, best_threshold, mixed_labels)\n",
    "print(f\"Final Evaluation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piston",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
